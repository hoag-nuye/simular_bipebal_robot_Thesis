# === Information ================================================================
#  @author:       Ho√†ng Nguy√™n
#  Email:         nguyen80o08nguyen@gmail.com
#  Github:        hoag.nuye
#  Created Date:  2024-12-07
# === Information ================================================================
import os
import pickle
import time

import torch
import numpy as np

from interface.progress_console import data_processing_console


# ========================== SUPERCLASS =================


class Buffer:
    def __init__(self):
        self.buffer = {
            "states": [],
            "actions": [],
            "rewards": [],
            "log_probs": [],
            "values": [],
            "trajectory_ids": [],
        }

    def add_sample(self, state, action, reward, log_prob, value, trajectory_id):
        """Th√™m m·ªôt m·∫´u v√†o buffer nh·ªè (RAM)."""
        self.buffer["states"].append(state)
        self.buffer["actions"].append(action)
        self.buffer["rewards"].append(reward)
        self.buffer["log_probs"].append(log_prob)
        self.buffer["values"].append(value)
        self.buffer["trajectory_ids"].append(trajectory_id)

    def get_samples(self):
        return self.buffer


# ========= REPLAY C√ì K√çCH TH∆Ø·ªöC NH·ªé ƒê·ªÇ CH·∫†Y ƒêA LU·ªíNG =============
class ReplayCache(Buffer):
    def __init__(self):
        # G·ªçi h√†m __init__ c·ªßa class Buffer ƒë·ªÉ kh·ªüi t·∫°o buffer
        super().__init__()

    # Set id cho trajectory
    def sget_range_trajectory(self, begin_id=0):
        self.buffer["trajectory_ids"] = [_id+begin_id for _id in self.buffer["trajectory_ids"]]
        last_id = max(self.buffer["trajectory_ids"])+1
        return last_id

# ========================== RELAY BUFFER L·ªöN =======================


class ReplayBuffer(Buffer):
    def __init__(self,
                 trajectory_size=32 * 300,
                 max_size=50000,
                 gamma=0.99, lam=0.97, alpha=0.6,
                 file_path="replay_buffer"
                           ".pkl"):
        """
        trajectory_size: S·ªë l∆∞·ª£ng m·∫´u c·∫ßn thu th·∫≠p m·ªói l·∫ßn hu·∫•n luy·ªán.
        max_size: K√≠ch th∆∞·ªõc t·ªëi ƒëa c·ªßa buffer khi l∆∞u to√†n b·ªô v√†o ·ªï c·ª©ng.
        """

        self.gamma = gamma
        self.lam = lam
        self.alpha = alpha
        # Theo d√µi TD error qua c√°c l·∫ßn train
        self.mean_td_error = None  # TD err hi·ªán t·∫°i
        self.history_length = 10
        self.td_error_history = []
        self.file_path = file_path  # ƒê∆∞·ªùng d·∫´n l∆∞u buffer v√†o ·ªï c·ª©ng

        # G·ªçi h√†m __init__ c·ªßa class Buffer ƒë·ªÉ kh·ªüi t·∫°o buffer
        super().__init__()
        # Kh·ªüi t·∫°o th√™m c√°c gi√° tr·ªã c·∫ßn l∆∞u kh√°c
        self.buffer["returns"] = []
        self.buffer["advantages"] = []
        self.buffer["td_errors"] = []

        # Ki·ªÉm tra n·∫øu file t·ªìn t·∫°i v√† x√≥a n√≥
        if os.path.exists(file_path):
            os.remove(file_path)
            print(f"File {file_path} ƒë√£ ƒë∆∞·ª£c x√≥a.")
        else:
            print(f"File {file_path} kh√¥ng t·ªìn t·∫°i.")

    def reset(self):
        """Reset buffer trong RAM."""
        for key in self.buffer:
            self.buffer[key] = []

    def buffer_add_sample(self, state, action, reward, log_prob, value,
                          returns, advantages, td_errors,
                          trajectory_id):
        """Th√™m m·ªôt m·∫´u v√†o buffer nh·ªè (RAM).
        :param returns:
        :param advantages:
        :param td_errors:
        """
        self.buffer["states"].append(state)
        self.buffer["actions"].append(action)
        self.buffer["rewards"].append(reward)
        self.buffer["log_probs"].append(log_prob)
        self.buffer["values"].append(value)
        self.buffer["returns"].append(returns)
        self.buffer["advantages"].append(advantages)
        self.buffer["td_errors"].append(td_errors)
        self.buffer["trajectory_ids"].append(trajectory_id)

    def append_from_buffer(self, buffer):
        """
        Th√™m to√†n b·ªô d·ªØ li·ªáu t·ª´ buffer ƒë·∫ßu v√†o v√†o buffer hi·ªán t·∫°i v√† t√≠nh to√°n l·∫°i returns, advantages, TD errors.

        Args:
            buffer (dict): Buffer ƒë·∫ßu v√†o ph·∫£i c√≥ c·∫•u tr√∫c t∆∞∆°ng t·ª± v·ªõi buffer hi·ªán t·∫°i.
        """
        # Ki·ªÉm tra xem buffer ƒë·∫ßu v√†o c√≥ ƒë·∫ßy ƒë·ªß c√°c kh√≥a kh√¥ng
        required_keys = ["states", "actions", "rewards", "log_probs", "values", "trajectory_ids"]
        for key in required_keys:
            if key not in buffer:
                raise ValueError(f"Buffer ƒë·∫ßu v√†o thi·∫øu kh√≥a: {key}")

        # T√≠nh to√°n returns, advantages, v√† TD errors cho buffer ƒë·∫ßu v√†o
        returns, advantages = self.compute_returns_and_advantages(buffer)
        td_errors = self.update_td_errors(buffer)

        # Duy·ªát qua t·ª´ng m·∫´u trong buffer ƒë·∫ßu v√†o v√† th√™m v√†o buffer hi·ªán t·∫°i
        for i in range(len(buffer["states"])):  # Gi·∫£ ƒë·ªãnh m·ªçi danh s√°ch ƒë·ªÅu c√≥ c√πng ƒë·ªô d√†i
            self.buffer_add_sample(state=buffer["states"][i],
                                   action=buffer["actions"][i],
                                   reward=buffer["rewards"][i],
                                   log_prob=buffer["log_probs"][i],
                                   value=buffer["values"][i],
                                   returns=returns[i],
                                   advantages=advantages[i],
                                   td_errors=td_errors[i],
                                   trajectory_id=buffer["trajectory_ids"][i])

    # =========================== T√çNH At v√† Gt ========================
    def compute_returns_and_advantages(self, buffer):
        """T√≠nh to√°n returns v√† advantages cho buffer ƒë·∫ßu v√†o theo GAE (\u03bb=1)."""
        T = len(buffer["rewards"])
        returns = [0] * T
        advantages = [0] * T

        next_value = 0  # Gi√° tr·ªã V(s_T) (·ªü ngo√†i trajectory)
        next_advantage = 0

        for t in reversed(range(T)):
            # Ki·ªÉm tra ng·∫Øt trajectory
            if t == T - 1 or (buffer["trajectory_ids"][t] != buffer["trajectory_ids"][t + 1]):
                next_value = 0  # Reset gi√° tr·ªã V(s) khi ng·∫Øt trajectory
                next_advantage = 0

            delta = buffer["rewards"][t] + self.gamma * next_value - buffer["values"][t]
            advantages[t] = delta + self.gamma * self.lam * next_advantage
            returns[t] = advantages[t] + buffer["values"][t]

            next_value = buffer["values"][t]
            next_advantage = advantages[t]
        return returns, advantages

    #  =============== T√≠nh to√°n TD errors =====================
    def update_td_errors(self, buffer):
        """T√≠nh to√°n TD-error cho buffer ƒë·∫ßu v√†o."""
        td_errors = []
        for t in range(len(buffer["rewards"])):
            reward = buffer["rewards"][t]
            value = buffer["values"][t]
            next_value = buffer["values"][t + 1] if t + 1 < len(buffer["rewards"]) else 0
            td_error = reward + self.gamma * next_value - value
            td_errors.append(td_error)

        self.mean_td_error = np.mean(td_errors)
        self.td_error_history.append(self.mean_td_error)
        # # --------- KI·ªÇM TRA VI·ªÜC T√çNH TD ERR -----------
        # td_err = []
        # r = []
        # v = []
        # for idx, t in enumerate(buffer["trajectory_ids"]):
        #     if t == 0:
        #         r.append(np.array(buffer["rewards"])[idx])
        #         v.append(np.array(buffer["values"])[idx])
        # print(f"td_er_calculator: {td_errors}")
        # print(f"reward: {r}")
        # print(f"values: {v}")
        return td_errors

    # == ki·ªÉm tra over-under fitting--
    def detect_overfitting_or_underfitting(self):
        """
        Ph√°t hi·ªán overfitting ho·∫∑c underfitting d·ª±a tr√™n TD-error trung b√¨nh v√† l·ªãch s·ª≠.
        - Overfitting: TD-error gi·∫£m m·∫°nh v√† ·ªïn ƒë·ªãnh (gi√° tr·ªã r·∫•t th·∫•p so v·ªõi l·ªãch s·ª≠).
        - Underfitting: TD-error tƒÉng ƒë·ªÅu ho·∫∑c kh√¥ng gi·∫£m trong l·ªãch s·ª≠.
        """
        # So s√°nh TD-error hi·ªán t·∫°i v·ªõi l·ªãch s·ª≠
        if len(self.td_error_history) > 1:
            mean_error = np.mean(self.td_error_history)
            std_error = np.std(self.td_error_history)
            normalized_error = (self.mean_td_error - mean_error) / (std_error + 1e-6)

            # print(normalized_error)

            if normalized_error < -2:
                return "overfitting"
            elif normalized_error > 2:
                return "underfitting"

        return "normal"

    # ====================== T·∫†O RA C√ÅC MINIBATCH ================

    # ====================== L·∫§Y D·ªÆ LI·ªÜU CHO TRAINING ============
    # S·ª¨ D·ª§NG CHO NH·ªÆNG THU·∫¨T TO√ÅN ON-POLICY (HU·∫§N LUY·ªÜN T·ª™ STATE HI·ªÜN T·∫†I)
    def sample_batch(self, batch_size=32,):
        mini_batch_dict = []

        # T·∫°o c√°c nh√≥m t∆∞∆°ng ·ª©ng cho trajectory_ids v·ªõi ch·ªâ m·ª•c
        trajectory_ids = self.buffer["trajectory_ids"]

        # L·∫•y c√°c gi√° tr·ªã duy nh·∫•t t·ª´ trajectory_ids v√† random h√≥a
        unique_trajectories = np.unique(trajectory_ids)
        # print(unique_trajectories)
        if batch_size > len(unique_trajectories):
            raise ValueError(f"Batch size ({batch_size}) l·ªõn h∆°n s·ªë trajectory kh·∫£ d·ª•ng ({len(unique_trajectories)}).")

        # X√°o tr·ªôn c√°c gi√° tr·ªã duy nh·∫•t
        # np.random.shuffle(unique_trajectories)

        # T·∫°o danh s√°ch ch·ª©a c√°c nh√≥m v·ªõi m·ªói nh√≥m c√≥ len_batch gi√° tr·ªã duy nh·∫•t c·ªßa buffer
        grouped_trajectories = [unique_trajectories[i:i + batch_size].tolist() for i in
                                range(0, len(unique_trajectories), batch_size)]
        # print(grouped_trajectories)
        # L·∫•y c√°c ch·ªâ m·ª•c t∆∞∆°ng ·ª©ng ƒë∆∞·ª£c chia theo grouped_trajectories
        batches_indices = []
        grouped_trajectories_size = len(grouped_trajectories)
        start_time = time.time()
        for idx, group in enumerate(grouped_trajectories):
            data_processing_console(total_steps=grouped_trajectories_size,
                                    current_steps=idx + 1,
                                    begin_time=start_time)
            group_indices = np.where(np.isin(trajectory_ids, group))[0]  # Ch·ªâ m·ª•c t∆∞∆°ng ·ª©ng v·ªõi group
            batches_indices.append(group_indices)

        # L·∫•y c√°c gi√° tr·ªã trong buffer theo c√°c ch·ªâ m·ª•c ƒë√£ ch·ªçn
        batches_indices_size = len(batches_indices)
        start_time = time.time()
        # Chuy·ªÉn self.buffer sang NumPy array m·ªôt l·∫ßn duy nh·∫•t
        numpy_buffer = {key: np.array(value) for key, value in self.buffer.items()}

        for idx, batch_indices in enumerate(batches_indices):
            data_processing_console(total_steps=batches_indices_size,
                                    current_steps=idx + 1,
                                    begin_time=start_time)

            # Truy xu·∫•t d·ªØ li·ªáu nhanh h∆°n t·ª´ numpy_buffer
            batch_dict = {key: numpy_buffer[key][batch_indices] for key in numpy_buffer}
            mini_batch_dict.append(batch_dict)

        # for idx, batch_indices in enumerate(batches_indices):
        #     data_processing_console(total_steps=batches_indices_size,
        #                             current_steps=idx + 1,
        #                             begin_time=start_time)
        #     batch_dict = {key: np.array([]) for key in buffer.keys()}
        #     for key in buffer.keys():
        #         batch_dict[key] = np.array(buffer[key])[batch_indices]
        #     mini_batch_dict.append(batch_dict)

        # ===== T·∫†O ƒê·∫¶U V√ÄO CHO VI·ªÜC HU·∫§N LUY·ªÜN THEO C√ÅC BATCH ƒê√É CHIA ==============
        """
        ƒê·∫¶U RA L√Ä C√ÅC BATCH C√ì CH·ª®A D·ªÆ LI·ªÜU C·ª¶A C√ÅC
        TENSOR C√ì SHAPE L√Ä : [batch_size, num_samples, feature_size]
        """
        mini_batch = []
        len_mini_batch_dict = len(mini_batch_dict)
        start_time = time.time()
        for idx, _batch_dict in enumerate(mini_batch_dict):
            data_processing_console(total_steps=len_mini_batch_dict,
                                    current_steps=idx+1,
                                    begin_time=start_time)
            # Kh·ªüi t·∫°o dictionary ch·ª©a c√°c batch ƒë√£ ƒë∆∞·ª£c chia
            batch = {}
            # Kh·ªüi t·∫°o gi√° tr·ªã l∆∞u ƒë·ªô d√†i th·ª±c t·∫ø c·ªßa t·ª´ng traj trong batch
            lengths_traj = None
            # Duy·ªát qua t·∫•t c·∫£ c√°c keys v√† chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh tensor
            trajectory_ids = _batch_dict["trajectory_ids"]  # D·ªØ li·ªáu trajectory_ids
            # print(len(np.unique(trajectory_ids)))
            # print(np.unique(trajectory_ids))
            for key in _batch_dict.keys():
                if key != "trajectory_ids":
                    unique_trajectories = np.unique(trajectory_ids)
                    # L·∫•y li·ªáu cho key hi·ªán t·∫°i
                    data = _batch_dict[key]
                    # if key == "rewards":
                    #     for idx, d in enumerate(data):
                    #         if _batch_dict["trajectory_ids"][idx] == _batch_dict["trajectory_ids"][1]:
                    #             print(data[idx])
                    # T·∫°o m·ªôt m·∫£ng ch·ª©a mask cho m·ªói trajectory_id
                    mask_list = [trajectory_ids == trajectory_id for trajectory_id in unique_trajectories]

                    # D√πng broadcasting v√† slicing ƒë·ªÉ nh√≥m d·ªØ li·ªáu theo trajectory_id
                    grouped_data = [data[mask] for mask in mask_list]

                    # T√¨m s·ªë m·∫´u l·ªõn nh·∫•t trong m·ªói nh√≥m ƒë·ªÉ padding
                    max_samples = max(len(group) for group in grouped_data)

                    # lengths_traj: Danh s√°ch ƒë·ªô d√†i th·ª±c c·ªßa t·ª´ng trajectory trong batch
                    lengths_traj = torch.tensor([len(group) for group in grouped_data])

                    # ƒê∆ØA V·ªÄ ARR 1 SANG V·ªÄ 2 CHI·ªÄU ƒê·ªÇ PADDING
                    for i, group in enumerate(grouped_data):
                        if len(group.shape) != 2:
                            grouped_data[i] = group.reshape(-1, 1)  # G√°n l·∫°i group v√†o grouped_data

                    # Padding d·ªØ li·ªáu sao cho t·∫•t c·∫£ c√°c nh√≥m ƒë·ªÅu c√≥ s·ªë l∆∞·ª£ng m·∫´u nh∆∞ nhau
                    # (0, max_samples - len(group)), (0, 0) => m·∫£ng ƒë∆∞·ª£c padding ph·∫£i l√† m·∫£ng 2 chi·ªÅu
                    padded_data = np.array(
                        [np.pad(group,
                                ((0, max_samples - len(group)), (0, 0)),
                                mode='constant') for group in grouped_data])

                    # Chuy·ªÉn th√†nh tensor
                    batch[key] = torch.tensor(padded_data)
                    batch["lengths_traj"] = lengths_traj
            mini_batch.append(batch)
        return mini_batch

    # S·ª¨ D·ª§NG CHO NH·ªÆNG THU·∫¨T TO√ÅN OFF-POLICY (HU·∫§N LUY·ªÜN T·ª™ C√ÅC D·ªÆ LI·ªÜU KH√ÅC)
    # def sample_batch_use_buffer(self, batch_size=32):
    #     """
    #     Sampling batch d·ª±a tr√™n tr·∫°ng th√°i overfitting ho·∫∑c underfitting, t·ª´ buffer l·ªõn ƒë∆∞·ª£c t·∫£i t·ª´ ·ªï c·ª©ng.
    #     """
    #     batch_result = {}  # l∆∞u k·∫øt qu·∫£
    #     lengths_traj = None  # l∆∞u k·∫øt qu·∫£ v·ªã tr√≠ c√°c sample v√† sampling
    #     selected_trajectory_ids = []
    #     # T·∫£i d·ªØ li·ªáu t·ª´ ·ªï c·ª©ng (buffer l·ªõn)
    #     buffer_large = self.load_from_pkl()
    #
    #     # 1. L·∫•y danh s√°ch c√°c gi√° tr·ªã duy nh·∫•t trong "trajectory_ids"
    #     unique_trajectory_ids = np.unique(buffer_large["trajectory_ids"])
    #     num_trajectories = len(unique_trajectory_ids)
    #     # ƒê·∫£m b·∫£o batch size kh√¥ng v∆∞·ª£t qu√° s·ªë trajectory c√≥ s·∫µn
    #     if batch_size > num_trajectories:
    #         raise ValueError(f"Batch size ({batch_size}) l·ªõn h∆°n s·ªë trajectory kh·∫£ d·ª•ng ({num_trajectories}).")
    #
    #     # 2. Chuy·ªÉn d·ªØ li·ªáu v·ªÅ array
    #     batch_dict = {
    #         "states": np.array(buffer_large["states"]),  # 2D
    #         "actions": np.array(buffer_large["actions"]),  # 2D
    #         "log_probs": np.array(buffer_large["log_probs"]).reshape(-1, 1),  # 1D -> 2D
    #         "rewards": np.array(buffer_large["rewards"]).reshape(-1, 1),  # 1D -> 2D
    #         "returns": np.array(buffer_large["returns"]).reshape(-1, 1),  # 1D -> 2D
    #         "advantages": np.array(buffer_large["advantages"]).reshape(-1, 1),  # 1D -> 2D
    #         "td_errors": np.array(buffer_large["td_errors"]).reshape(-1, 1),  # 1D -> 2D
    #         # 1D -> 2D
    #         "trajectory_ids": np.array(buffer_large["trajectory_ids"])}
    #
    #     # print(batch_dict["trajectory_ids"].shape, batch_dict["states"].shape, np.array(buffer_large["states"]).shape)
    #
    #     # 3. Kh·ªüi t·∫°o dictionary k·∫øt qu·∫£
    #     batch_large = {}
    #     # Duy·ªát qua t·∫•t c·∫£ c√°c keys v√† chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu th√†nh tensor
    #     trajectory_ids = batch_dict["trajectory_ids"]  # D·ªØ li·ªáu trajectory_ids
    #     for key in batch_dict.keys():
    #         if key != "trajectory_ids":
    #
    #             data = np.array(batch_dict[key])  # Chuy·ªÉn d·ªØ li·ªáu sang m·∫£ng NumPy n·∫øu c·∫ßn  # D·ªØ li·ªáu cho key hi·ªán t·∫°i
    #             # print(key, batch_dict[key].shape)
    #             # T·∫°o m·ªôt m·∫£ng ch·ª©a mask cho m·ªói trajectory_id
    #             mask_list = np.array([trajectory_ids == trajectory_id for trajectory_id in unique_trajectory_ids])
    #             # print(data.shape, mask_list.shape)
    #
    #             grouped_data = [data[mask] for mask in mask_list]
    #
    #             # T√¨m s·ªë m·∫´u l·ªõn nh·∫•t trong m·ªói nh√≥m ƒë·ªÉ padding
    #             max_samples = max(len(group) for group in grouped_data)
    #
    #             # # Ki·ªÉm tra k√≠ch th∆∞·ªõc c·ªßa t·ª´ng group tr∆∞·ªõc khi padding
    #             # print(f"Max samples: {max_samples}")
    #             # print(f"Grouped data shapes before padding: {[group.shape for group in grouped_data]}")
    #
    #             # Padding d·ªØ li·ªáu sao cho t·∫•t c·∫£ c√°c nh√≥m ƒë·ªÅu c√≥ s·ªë l∆∞·ª£ng m·∫´u nh∆∞ nhau
    #             padded_data = np.array(
    #                 [np.pad(group, ((0, max_samples - len(group)), (0, 0)), mode='constant', constant_values=-1) for
    #                  group in grouped_data])
    #
    #             # Chuy·ªÉn th√†nh tensor
    #             batch_large[key] = torch.tensor(padded_data)
    #
    #     # print(batch_large["states"].shape)
    #     # print("TENSOR:", result["states"][-1, 0, :])
    #     # print("DEQUE:", [buffer_large["states"][i] for i in range(len(buffer_large["trajectory_ids"]))
    #     #                  if buffer_large["trajectory_ids"][i] == unique_trajectory_ids[-1]])
    #     # ============================================================
    #     # =================== L·ªåC D·ªÆ LI·ªÜU ============================
    #     # =============================================================
    #
    #     # Ph√°t hi·ªán tr·∫°ng th√°i hu·∫•n luy·ªán
    #     status = self.detect_overfitting_or_underfitting()
    #
    #     # ========== L·∫§Y D·ªÆ LI·ªÜU KHI G·∫∂P PH·∫¢I OVER-UNDER FIT================
    #     if status in ["overfitting", "underfitting"]:
    #         # print(status)
    #         # 90% trajectory l·∫•y t·ª´ d·ªØ li·ªáu m·ªõi nh·∫•t
    #         percent = 0.9
    #         batch_main_size = int(batch_size * percent)
    #         bath_rest_size = int(batch_size - batch_main_size)  # S·ªë l∆∞·ª£ng c·∫ßn l·∫•y
    #
    #         main_trajectory_ids = unique_trajectory_ids[-batch_main_size:]
    #         rest_trajectory_ids = unique_trajectory_ids[:-batch_main_size]
    #
    #         # print(main_trajectory_ids, rest_trajectory_ids)
    #
    #         if batch_main_size != batch_size:
    #             batch_main = {key: tensor[torch.tensor(main_trajectory_ids)] for key, tensor in batch_large.items()}
    #             rest_trajectory = {key: tensor[torch.tensor(rest_trajectory_ids)] for key, tensor in
    #                                batch_large.items()}  # d·ªØ li·ªáu ƒë∆∞·ª£c ph√©p l·∫•y
    #             # B∆∞·ªõc 1: Truy xu·∫•t rewards v√† t√≠nh priorities
    #             td_errors = rest_trajectory["td_errors"]  # ex: Shape: [32, 300, 1]
    #             mean_td_errors = td_errors.mean(dim=1, keepdim=False)  # Shape: [32, 1]
    #             # TD Error (ùõø) c√≥ th·ªÉ d∆∞∆°ng ho·∫∑c √¢m
    #             # nh∆∞ng ch·ªâ c·∫ßn quan t√¢m ƒë·∫øn ƒë·ªô l·ªõn c·ªßa sai l·ªách (absolute difference), kh√¥ng c·∫ßn quan t√¢m ƒë·∫øn d·∫•u.
    #             priorities = torch.abs(mean_td_errors).squeeze(
    #                 -1) ** self.alpha  # Shape: [32] # priorities m≈© l√™n ƒë·ªÉ th·ªÉ hi·ªán s·ª± quan tr·ªçng (∆∞u ti√™n)
    #             # B∆∞·ªõc 2: Normalize probabilities
    #             probabilities = priorities / torch.sum(priorities)
    #             # ƒê·∫£m b·∫£o t·ªïng x√°c su·∫•t = 1 b·∫±ng c√°ch chu·∫©n h√≥a l·∫°i n·∫øu c·∫ßn
    #             if not np.isclose(probabilities.sum().item(), 1.0, atol=1e-6):
    #                 probabilities = probabilities / probabilities.sum()
    #             # B∆∞·ªõc 3: Ch·ªçn num_choice ch·ªâ m·ª•c theo x√°c su·∫•t ∆∞u ti√™n
    #             selected_indices = np.random.choice(
    #                 rest_trajectory_ids, size=bath_rest_size, replace=False, p=probabilities)
    #             # B∆∞·ªõc 4: Tr√≠ch xu·∫•t tensor t∆∞∆°ng ·ª©ng t·ª´ result
    #             batch_rest = {key: value[selected_indices] for key, value in rest_trajectory.items()}
    #             # B∆∞·ªõc 5: G·ªôp batch_main v√† batch_rest
    #             selected_trajectory_ids = np.concatenate((selected_indices, main_trajectory_ids))
    #             batch_result = {key: torch.cat([batch_main[key], batch_rest[key]], dim=0)
    #                             for key in batch_large.keys()}
    #
    #         else:
    #             # T·∫°o dictionary m·ªõi ch·ª©a c√°c tensor ƒë√£ l·ªçc
    #             selected_trajectory_ids = main_trajectory_ids
    #             batch_result = {key: tensor[torch.tensor(main_trajectory_ids)] for key, tensor in batch_large.items()}
    #     # ========== L·∫§Y D·ªÆ LI·ªÜU NH∆Ø B√åNH TH∆Ø·ªúNG================
    #     else:
    #         # print(status)
    #         # N·∫øu overfitting ho·∫∑c underfitting:
    #         # 90% trajectory l·∫•y t·ª´ d·ªØ li·ªáu m·ªõi nh·∫•t
    #         percent = 0.5
    #         batch_main_size = int(batch_size * percent)
    #         bath_rest_size = int(batch_size - batch_main_size)  # S·ªë l∆∞·ª£ng c·∫ßn l·∫•y
    #
    #         main_trajectory_ids = unique_trajectory_ids[-batch_main_size:]
    #         rest_trajectory_ids = unique_trajectory_ids[:-bath_rest_size]
    #
    #         if batch_main_size != batch_size:
    #             batch_main = {key: tensor[torch.tensor(main_trajectory_ids)] for key, tensor in batch_large.items()}
    #             rest_trajectory = {key: tensor[torch.tensor(rest_trajectory_ids)] for key, tensor in
    #                                batch_large.items()}  # d·ªØ li·ªáu ƒë∆∞·ª£c ph√©p l·∫•y
    #             # B∆∞·ªõc 1: Truy xu·∫•t reward v√† t√≠nh priorities
    #             rewards = rest_trajectory["rewards"]  # Shape: [32, 300, 1]
    #             mean_rewards = rewards.mean(dim=1, keepdim=False)  # Shape: [32, 1]
    #             # ƒë·∫£m b·∫£o ph·∫ßn th∆∞·ªüng trung b√¨nh d∆∞∆°ng nh∆∞ng v·∫´n x·∫øp theo th·ª© t·ª±
    #             min_reward = mean_rewards.min().item()  # Gi√° tr·ªã ph·∫ßn th∆∞·ªüng nh·ªè nh·∫•t
    #             epsilon = 1e-6  # Gi√° tr·ªã nh·ªè ƒë·ªÉ tr√°nh 0
    #             adjusted_rewards = mean_rewards - min_reward + 1e-6  # ƒê·∫£m b·∫£o kh√¥ng c√≥ gi√° tr·ªã √¢m
    #             priorities = adjusted_rewards.squeeze(-1) ** 2  # Shape: [32]
    #             # B∆∞·ªõc 2: Normalize probabilities
    #             probabilities = priorities / torch.sum(priorities)
    #             # ƒê·∫£m b·∫£o t·ªïng x√°c su·∫•t = 1 b·∫±ng c√°ch chu·∫©n h√≥a l·∫°i n·∫øu c·∫ßn
    #             if not np.isclose(probabilities.sum().item(), 1.0, atol=1e-6):
    #                 probabilities = probabilities / probabilities.sum()
    #             # B∆∞·ªõc 3: Ch·ªçn num_choice ch·ªâ m·ª•c theo x√°c su·∫•t ∆∞u ti√™n
    #             selected_indices = np.random.choice(
    #                 rest_trajectory_ids, size=bath_rest_size, replace=False, p=probabilities)
    #             # B∆∞·ªõc 4: Tr√≠ch xu·∫•t tensor t∆∞∆°ng ·ª©ng t·ª´ result
    #             batch_rest = {key: value[selected_indices] for key, value in rest_trajectory.items()}
    #             # B∆∞·ªõc 5: G·ªôp batch_main v√† batch_rest
    #             selected_trajectory_ids = np.concatenate((selected_indices, main_trajectory_ids))
    #             batch_result = {key: torch.cat([batch_main[key], batch_rest[key]], dim=0)
    #                             for key in batch_large.keys()}
    #         else:
    #             # T·∫°o dictionary m·ªõi ch·ª©a c√°c tensor ƒë√£ l·ªçc
    #             selected_trajectory_ids = main_trajectory_ids
    #             batch_result = {key: tensor[torch.tensor(main_trajectory_ids)] for key, tensor in batch_large.items()}
    #
    #     # ============================================================
    #     # =================== X·ª¨ L√ù L·∫†I PADDING ======================
    #     # ============================================================
    #     # --------------- T√åM LENGHT TH·∫¨T C·ª¶A TRAJECTORY ------
    #     # T·∫°o m·ªôt mask ƒë·ªÉ l·ªçc l·∫°i d·ªØ li·ªáu d·ª±a tr√™n c√°c trajectory_id trong batch_result
    #     mask_list_result = [batch_dict["trajectory_ids"] == traj_id for traj_id in selected_trajectory_ids]
    #
    #     # S·ª≠ d·ª•ng mask ƒë·ªÉ nh√≥m l·∫°i d·ªØ li·ªáu trong batch_dict (ho·∫∑c batch_large)
    #     grouped_data_result = [batch_dict["states"][mask] for mask in
    #                            mask_list_result]  # C√≥ th·ªÉ thay 'states' b·∫±ng key kh√°c
    #
    #     # T√≠nh l·∫°i ƒë·ªô d√†i th·ª±c s·ª± c·ªßa m·ªói trajectory trong batch_result
    #     lengths_traj = torch.tensor([len(group) for group in grouped_data_result])
    #     # ---------- T√åM SIZE PADDING TH·∫¨T C·ª¶A SAMPLE --------------
    #     max_lengths_traj = torch.max(lengths_traj)
    #     for key in batch_result.keys():
    #         batch_result[key] = batch_result[key][:, :max_lengths_traj, :]
    #     print(batch_result["states"].shape)
    #     # print(lengths_traj)
    #     return batch_result, lengths_traj
    #
    # # =================== L∆ØU TR·ªÆ D·ªÆ LI·ªÜU =====================
    # def save_to_pkl(self):
    #     """L∆∞u buffer v√†o file .pkl, x√≥a m·∫´u c≈© nh·∫•t n·∫øu v∆∞·ª£t qu√° max_size."""
    #     temp_file_path = self.file_path + ".tmp"
    #     current_buffer_size = len(self.buffer["states"])  # S·ªë m·∫´u trong RAM
    #
    #     if os.path.exists(self.file_path):
    #         print(f"File {self.file_path} t·ªìn t·∫°i.")
    #
    #     # Ghi d·ªØ li·ªáu m·ªõi v√†o file t·∫°m
    #     with open(temp_file_path, "wb") as temp_file:
    #         samples_written = 0
    #         try:
    #             # ƒê·ªçc file g·ªëc
    #             with open(self.file_path, "rb") as original_file:
    #                 while True:
    #                     try:
    #                         # Load m·ªôt ph·∫ßn d·ªØ li·ªáu t·ª´ file g·ªëc
    #                         data = pickle.load(original_file)
    #                         # X√≥a ph·∫ßn t·ª≠ c≈© n·∫øu v∆∞·ª£t qu√° max_size
    #                         for key in data:
    #                             while len(data[key]) > 0 and samples_written + len(
    #                                     data[key]) > self.max_size - current_buffer_size:
    #                                 data[key].popleft()
    #                         # Ghi ph·∫ßn d·ªØ li·ªáu c√≤n l·∫°i v√†o file t·∫°m
    #                         pickle.dump(data, temp_file)
    #                         samples_written += len(data["states"])
    #                     except EOFError:
    #                         break
    #         except FileNotFoundError:
    #             print("File g·ªëc kh√¥ng t·ªìn t·∫°i, t·∫°o file m·ªõi.")
    #
    #         # Ghi buffer nh·ªè (trong RAM) v√†o file t·∫°m
    #         pickle.dump(self.buffer, temp_file)
    #
    #     # Thay th·∫ø file g·ªëc b·∫±ng file t·∫°m
    #     os.replace(temp_file_path, self.file_path)
    #
    # def load_from_pkl(self):
    #     """T·∫£i to√†n b·ªô d·ªØ li·ªáu t·ª´ file .pkl."""
    #     buffer_large = {
    #         "states": [],
    #         "actions": [],
    #         "rewards": [],
    #         "log_probs": [],
    #         "returns": [],
    #         "advantages": [],
    #         "trajectory_ids": [],
    #         "td_errors": [],
    #     }
    #     try:
    #         with open(self.file_path, "rb") as f:
    #             while True:
    #                 try:
    #                     data = pickle.load(f)
    #                     for key in buffer_large:
    #                         buffer_large[key].extend(data[key])
    #                 except EOFError:
    #                     break
    #     except FileNotFoundError:
    #         print("File not found, starting with an empty buffer.")
    #     return buffer_large
